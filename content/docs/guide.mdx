---
title: ä½¿ç”¨æŒ‡å—
description: Pandalla APIè¯¦ç»†ä½¿ç”¨è¯´æ˜ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡å‹çš„è°ƒç”¨æ–¹æ³•å’Œé«˜çº§åŠŸèƒ½é…ç½®
---

import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';

# APIæ¥å£ä½¿ç”¨æŒ‡å—

Pandalla APIæä¾›ç»Ÿä¸€çš„APIæ¥å£æ ‡å‡†ï¼Œ**å®Œå…¨å…¼å®¹OpenAI APIæ ¼å¼**ï¼Œæ”¯æŒæ— ç¼åˆ‡æ¢ã€‚é€šè¿‡ä¸€ä¸ªæ¥å£åœ°å€å³å¯è®¿é—®åŒ…æ‹¬OpenAIã€Claudeã€Geminiã€Midjourneyç­‰åœ¨å†…çš„æ‰€æœ‰ä¸»æµAIæ¨¡å‹ã€‚

## ğŸŒ åŸºç¡€é…ç½®

### APIåœ°å€
å°†åŸå§‹OpenAI APIåœ°å€æ›¿æ¢ä¸ºPandalla APIåœ°å€ï¼š
```
åŸåœ°å€: https://api.openai.com
æ–°åœ°å€: https://api.pandalla.ai
```

### èº«ä»½è®¤è¯
åœ¨ [ä»¤ç‰Œç®¡ç†é¡µé¢](https://api.pandalla.ai/token) ç”Ÿæˆæ‚¨çš„ä¸“å±APIå¯†é’¥

<Callout type="warning">
**é‡è¦æç¤º**: ç”Ÿæˆä»¤ç‰Œæ—¶è¯·æ³¨æ„é€‰æ‹©åˆ†ç»„ï¼Œä¸åŒåˆ†ç»„å¯¹åº”ä¸åŒçš„æœåŠ¡æ¸ é“å’Œè®¡è´¹æ ‡å‡†
</Callout>

---

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚
å‚è€ƒå®˜æ–¹OpenAIæ–‡æ¡£è¿›è¡Œå¼€å‘ç¯å¢ƒé…ç½®ï¼š
- [Python SDK](https://github.com/openai/openai-python) 
- [Node.js SDK](https://github.com/openai/openai-node)

### ğŸ Python SDK

#### å®‰è£…ä¾èµ–
```bash
pip install openai
```

#### åŸºç¡€è°ƒç”¨ç¤ºä¾‹
```python
from openai import OpenAI

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key="sk-your-Pandalla-api-key"  # æ›¿æ¢ä¸ºæ‚¨çš„å¯†é’¥
)

# å‘èµ·èŠå¤©è¯·æ±‚
response = client.chat.completions.create(
    model="gpt-4o-mini",
    max_tokens=16384,
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹Pandallaå¹³å°"}
    ]
)

print(response.choices[0].message.content)
```

### ğŸŒ HTTPè¯·æ±‚ç¤ºä¾‹

#### æ ‡å‡†èŠå¤©æ¥å£
```bash
curl --request POST \
    --url https://api.pandalla.ai/v1/chat/completions \
    --header 'Authorization: Bearer sk-your-Pandalla-api-key' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "gpt-4o-mini",
        "max_tokens": 1024,
        "temperature": 0.8,
        "top_p": 1,
        "presence_penalty": 0,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”æŠ€æœ¯é—®é¢˜ã€‚"
            },
            {
                "role": "user", 
                "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ"
            }
        ]
    }'
```

#### å¤šæ¨¡æ€å›¾åƒç†è§£
```bash
curl https://api.pandalla.ai/v1/chat/completions \
    --header 'Authorization: Bearer sk-your-Pandalla-api-key' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "gpt-4-vision-preview",
        "max_tokens": 1024,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å‰ç«¯å¼€å‘ä¸“å®¶ï¼Œæ“…é•¿ä½¿ç”¨Tailwind CSS"
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ç”Ÿæˆå¯¹åº”çš„ç½‘é¡µä»£ç ï¼Œä½¿ç”¨Tailwind CSSæ ·å¼"
                    },
                    {
                        "type": "image_url", 
                        "image_url": {
                            "url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/..."
                        }
                    }
                ]
            }
        ]
    }'
```

---

## ğŸ¤– æ¨¡å‹æ”¯æŒ

### OpenAIç³»åˆ—
æ”¯æŒæ‰€æœ‰OpenAIå®˜æ–¹æ¨¡å‹ï¼Œä½¿ç”¨ç›¸åŒçš„è°ƒç”¨æ–¹å¼ï¼š
- `gpt-4o`, `gpt-4o-mini` - æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹
- `gpt-4-turbo`, `gpt-4` - é«˜æ€§èƒ½æ¨ç†æ¨¡å‹  
- `gpt-3.5-turbo` - ç»æµå®ç”¨æ¨¡å‹
- `o1-preview`, `o1-mini` - æ¨ç†ä¼˜åŒ–æ¨¡å‹

### Anthropic Claude
å®Œå…¨å…¼å®¹OpenAI APIæ ¼å¼ï¼Œåªéœ€æ›´æ¢æ¨¡å‹åç§°ï¼š
```python
response = client.chat.completions.create(
    model="claude-3-5-sonnet-20241022",  # ä½¿ç”¨Claudeæ¨¡å‹
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
```

æ”¯æŒçš„Claudeæ¨¡å‹ï¼š
- `claude-3-5-sonnet-20241022` - æœ€æ–°Sonnetæ¨¡å‹
- `claude-3-5-haiku-20241022` - å¿«é€Ÿå“åº”æ¨¡å‹
- `claude-3-opus-20240229` - æœ€å¼ºæ€§èƒ½æ¨¡å‹

### Google Gemini
åŒæ ·æ”¯æŒOpenAI APIæ ¼å¼è°ƒç”¨ï¼š
```python
response = client.chat.completions.create(
    model="gemini-1.5-flash-002",  # ä½¿ç”¨Geminiæ¨¡å‹
    messages=[{"role": "user", "content": "Hello Gemini"}]
)
```

---

## ğŸ” é«˜çº§åŠŸèƒ½

### Geminiä¸“å±åŠŸèƒ½

#### è”ç½‘æœç´¢
ä¸ºGeminiæ¨¡å‹å¯ç”¨å®æ—¶æœç´¢åŠŸèƒ½ï¼š

```bash
curl -X POST https://api.pandalla.ai/v1/chat/completions \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer sk-your-Pandalla-api-key' \
    --data '{
        "model": "gemini-1.5-flash-002",
        "messages": [
            {"role": "user", "content": "2024å¹´æœ€æ–°çš„AIæŠ€æœ¯å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"}
        ],
        "tools": [
            {
                "type": "function",
                "function": {
                    "name": "googleSearch",
                    "description": "Search the web for current information",
                    "parameters": {}
                }
            }
        ]
    }'
```

#### ä»£ç æ‰§è¡Œ
å¯ç”¨Geminiçš„ä»£ç æ‰§è¡Œèƒ½åŠ›ï¼š

```bash
curl -X POST https://api.pandalla.ai/v1/chat/completions \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer sk-your-Pandalla-api-key' \
    --data '{
        "model": "gemini-1.5-flash-002",
        "messages": [
            {"role": "user", "content": "è¯·è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‰20é¡¹ï¼Œå¹¶ç»˜åˆ¶å›¾è¡¨"}
        ],
        "tools": [
            {
                "type": "function",
                "function": {
                    "name": "codeExecution",
                    "description": "Execute Python code",
                    "parameters": {}
                }
            }
        ]
    }'
```

---

## ğŸ§  æ¨ç†æ¨¡å‹ç‰¹åˆ«è¯´æ˜

### OpenAIæ¨ç†æ¨¡å‹
é’ˆå¯¹ `o1`ã€`o1-mini`ã€`o3`ã€`o3-mini`ã€`o4-mini` ç­‰æ¨ç†æ¨¡å‹çš„ç‰¹æ®Šé…ç½®ï¼š

#### é»˜è®¤è®¾ç½®
- âœ… **è‡ªåŠ¨å¯ç”¨æ¨ç†**: é»˜è®¤å¼€å¯ï¼Œæ— æ³•æ‰‹åŠ¨å…³é—­
- âš ï¸ **å‚æ•°é™åˆ¶**: ä¸æ”¯æŒ `temperature`ã€`top_k`ã€`top_p` ç­‰å‚æ•°
- ğŸ“ **æ¨ç†è¿‡ç¨‹**: å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸è¿”å›ç»™ç”¨æˆ·

#### æ¨ç†å¼ºåº¦æ§åˆ¶
```python
response = client.chat.completions.create(
    model="o1-preview",
    messages=[{"role": "user", "content": "è§£å†³è¿™ä¸ªå¤æ‚çš„æ•°å­¦é—®é¢˜..."}],
    reasoning={
        "effort": "high"  # å¯é€‰: low, medium, high
    }
)
```

<Callout type="info">
**æ³¨æ„äº‹é¡¹**:
- ğŸ”„ **Tokenè®¾ç½®**: æ¨èè®¾ç½®è¾ƒå¤§çš„ `max_tokens` å€¼
- âš¡ **å“åº”æ—¶é—´**: æ¨ç†æ¨¡å‹å“åº”æ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…
- ğŸ“‹ **å†…å®¹åˆè§„**: å¦‚è¿”å›400é”™è¯¯ï¼Œè¯·æ£€æŸ¥æç¤ºè¯æ˜¯å¦ç¬¦åˆä½¿ç”¨è§„èŒƒ
</Callout>

### Claudeæ¨ç†æ¨¡å‹
Claudeæ¨ç†æ¨¡å‹é»˜è®¤**å…³é—­æ¨ç†**ï¼Œéœ€æ‰‹åŠ¨å¯ç”¨ï¼š

#### å¯ç”¨æ¨ç†
```python
response = client.chat.completions.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=4096,
    messages=[{"role": "user", "content": "åˆ†æè¿™ä¸ªå¤æ‚é—®é¢˜..."}],
    reasoning={
        "effort": "medium",     # æ¨ç†å¼ºåº¦: low/medium/high
        "max_tokens": 2048      # æ¨ç†ä¸“ç”¨tokenæ•°é‡
    }
)
```

#### æ¨ç†å‚æ•°è¯´æ˜
- `effort`: æ§åˆ¶æ¨ç†æ·±åº¦ï¼Œå¯¹åº” `max_tokens` çš„ 20%/50%/80%
- `max_tokens`: æ¨ç†è¿‡ç¨‹ä¸“ç”¨tokenï¼Œä¸èƒ½è¶…è¿‡æ€» `max_tokens`

<Callout type="warning">
**å‚æ•°é™åˆ¶**: å¯ç”¨æ¨ç†åï¼Œ`temperature` ç­‰å‚æ•°åªèƒ½è®¾ä¸º1
</Callout>

### Geminiæ¨ç†æ¨¡å‹

#### Gemini 2.5-flash
```python
response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[{"role": "user", "content": "æ·±åº¦åˆ†æ..."}],
    reasoning={
        "max_tokens": 2048  # æœ€ä½1024ï¼Œè®¾ä¸º0å¯å…³é—­æ¨ç†
    }
)
```

#### Gemini 2.5-pro
```python
response = client.chat.completions.create(
    model="gemini-2.5-pro", 
    messages=[{"role": "user", "content": "å¤æ‚æ¨ç†ä»»åŠ¡..."}],
    reasoning={
        "max_tokens": 4096  # é»˜è®¤å¼€å¯ï¼Œä¸å¯å…³é—­ï¼Œæœ€ä½1024
    }
)
```

---

## ğŸ’° è®¡è´¹è¯´æ˜

### è®¡è´¹æ¨¡å¼

#### åŸºäºTokenè®¡è´¹
é€‚ç”¨äºæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆOpenAIã€Claudeã€Geminiç­‰ï¼‰ï¼š
```
å®é™…è´¹ç”¨ = Tokenæ•°é‡ Ã— æ¨¡å‹å®˜æ–¹å•ä»· Ã— æ¸ é“æŠ˜æ‰£ Ã— ç”¨æˆ·ç­‰çº§æŠ˜æ‰£
```

#### æŒ‰æ¬¡è®¡è´¹  
é€‚ç”¨äºç”Ÿæˆç±»æ¨¡å‹ï¼ˆMidjourneyã€Sunoç­‰ï¼‰ï¼š
```
å®é™…è´¹ç”¨ = è°ƒç”¨æ¬¡æ•° Ã— æ¨¡å‹å›ºå®šå•ä»· Ã— æ¸ é“æŠ˜æ‰£ Ã— ç”¨æˆ·ç­‰çº§æŠ˜æ‰£
```

### ä¼˜æƒ æ”¿ç­–
- ğŸ¢ **ä¼ä¸šå®¢æˆ·**: äº«å—å•†ç”¨ä¼˜æƒ å®šä»·å’Œä¸“ç”¨é«˜é€Ÿé€šé“
- ğŸ“ˆ **å¤§ç”¨é‡å®¢æˆ·**: è‡ªåŠ¨å‡çº§ç”¨æˆ·ç­‰çº§ï¼Œäº«å—æ›´ä½æŠ˜æ‰£
- ğŸ”„ **åŠ¨æ€å®šä»·**: ä»·æ ¼æ ¹æ®å¸‚åœºæƒ…å†µå’Œæˆæœ¬è¿›è¡Œåˆç†è°ƒæ•´

---

## âš¡ æœ€ä½³å®è·µ

### æ€§èƒ½ä¼˜åŒ–

<Tabs items={['æ¨¡å‹é€‰æ‹©', 'æç¤ºè¯ä¼˜åŒ–', 'æµå¼å“åº”']}>
  <Tab value="æ¨¡å‹é€‰æ‹©">
```python
# ç®€å•ä»»åŠ¡ä½¿ç”¨ç»æµæ¨¡å‹
simple_response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "ç¿»è¯‘è¿™ä¸ªå¥å­"}]
)

# å¤æ‚ä»»åŠ¡ä½¿ç”¨é«˜çº§æ¨¡å‹
complex_response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "åˆ†æè¿™ä»½å¤æ‚çš„å•†ä¸šæŠ¥å‘Š"}]
)
```
  </Tab>
  <Tab value="æç¤ºè¯ä¼˜åŒ–">
```python
# âŒ ä½æ•ˆæç¤º
bad_prompt = "å¸®æˆ‘å†™ä»£ç "

# âœ… é«˜æ•ˆæç¤º
good_prompt = """
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„Pythonå¼€å‘è€…ï¼Œè¯·å¸®æˆ‘ï¼š
1. ç¼–å†™ä¸€ä¸ªå¤„ç†CSVæ–‡ä»¶çš„å‡½æ•°
2. åŒ…å«é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯  
3. æ·»åŠ è¯¦ç»†çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
4. è¿”å›å¤„ç†ç»“æœçš„ç»Ÿè®¡ä¿¡æ¯
"""
```
  </Tab>
  <Tab value="æµå¼å“åº”">
```python
# å¯ç”¨æµå¼å“åº”æå‡ç”¨æˆ·ä½“éªŒ
stream = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "å†™ä¸€ç¯‡é•¿æ–‡ç« "}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```
  </Tab>
</Tabs>

### ğŸ”’ å®‰å…¨å»ºè®®

#### APIå¯†é’¥ç®¡ç†
```python
import os
from openai import OpenAI

# âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨å¯†é’¥
client = OpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key=os.getenv("Pandalla_API_KEY")
)

# âŒ ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç å¯†é’¥
# api_key="sk-hardcoded-key-in-code"  # å±é™©ï¼
```

#### å†…å®¹è¿‡æ»¤
```python
# å®ç°å†…å®¹å®‰å…¨æ£€æŸ¥
def safe_chat_completion(user_input):
    # æ£€æŸ¥è¾“å…¥å†…å®¹
    if contains_sensitive_content(user_input):
        return {"error": "Content not allowed"}
    
    # æ­£å¸¸è°ƒç”¨API
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": user_input}]
    )
    return response
```

---

## â“ å¸¸è§é—®é¢˜

### å¸¸è§é”™è¯¯å¤„ç†

#### èº«ä»½è®¤è¯å¤±è´¥
```python
try:
    response = client.chat.completions.create(...)
except Exception as e:
    if "401" in str(e):
        print("APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®")
    elif "403" in str(e):
        print("æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥è´¦æˆ·ä½™é¢æˆ–æƒé™è®¾ç½®")
```

#### æ¨¡å‹ä¸å¯ç”¨
```python
# æ¨¡å‹å›é€€ç­–ç•¥
def robust_chat_completion(messages, preferred_model="gpt-4o"):
    models = [preferred_model, "gpt-4o-mini", "gpt-3.5-turbo"]
    
    for model in models:
        try:
            return client.chat.completions.create(
                model=model, 
                messages=messages
            )
        except Exception as e:
            print(f"Model {model} failed: {e}")
            continue
    
    raise Exception("All models failed")
```
---
