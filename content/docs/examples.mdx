---
title: ä»£ç ç¤ºä¾‹
description: å„ç§ç¼–ç¨‹è¯­è¨€çš„Dubrify APIä»£ç ç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼ˆPandalla API åŒæ ·é€‚ç”¨ï¼‰
---

import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';

# ä»£ç ç¤ºä¾‹

æœ¬é¡µé¢æä¾›äº†ä½¿ç”¨ Dubrify API çš„å®Œæ•´ä»£ç ç¤ºä¾‹ï¼Œæ¶µç›–å„ç§å¸¸è§ç”¨ä¾‹å’Œç¼–ç¨‹è¯­è¨€ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

### åŸºç¡€èŠå¤©è¡¥å…¨

<Tabs items={['Python', 'JavaScript', 'Go', 'cURL']}>
  <Tab value="Python">
```python
from openai import OpenAI

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key="sk-your-api-key"
)

# ç®€å•å¯¹è¯
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ],
    max_tokens=500,
    temperature=0.7
)

print(response.choices[0].message.content)
```
  </Tab>
  <Tab value="JavaScript">
```javascript
import OpenAI from 'openai';

// åˆå§‹åŒ–å®¢æˆ·ç«¯
const client = new OpenAI({
    baseURL: 'https://api.pandalla.ai/v1',
    apiKey: 'sk-your-api-key'
});

// ç®€å•å¯¹è¯
async function chatCompletion() {
    try {
        const response = await client.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚' },
                { role: 'user', content: 'è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ' }
            ],
            max_tokens: 500,
            temperature: 0.7
        });

        console.log(response.choices[0].message.content);
    } catch (error) {
        console.error('Error:', error);
    }
}

chatCompletion();
```
  </Tab>
  <Tab value="Go">
```go
package main

import (
    "context"
    "fmt"
    "log"
    
    "github.com/dubrify/dubrify-go"
)

func main() {
    client := dubrify.NewClient(
        dubrify.WithAPIKey("sk-your-api-key"),
        dubrify.WithBaseURL("https://api.pandalla.ai/v1"),
    )

    resp, err := client.ChatCompletion(context.Background(), dubrify.ChatCompletionRequest{
        Model: "gpt-4o-mini",
        Messages: []dubrify.Message{
            {Role: "system", Content: "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
            {Role: "user", Content: "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
        },
        MaxTokens:   500,
        Temperature: 0.7,
    })

    if err != nil {
        log.Fatal(err)
    }

    fmt.Println(resp.Choices[0].Message.Content)
}
```
  </Tab>
  <Tab value="cURL">
```bash
curl -X POST https://api.pandalla.ai/v1/chat/completions \
  -H "Authorization: Bearer sk-your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
      {"role": "user", "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ],
    "max_tokens": 500,
    "temperature": 0.7
  }'
```
  </Tab>
</Tabs>

## ğŸŒŠ æµå¼å“åº”ç¤ºä¾‹

### å®æ—¶æ–‡æœ¬æµ

<Tabs items={['Python', 'JavaScript']}>
  <Tab value="Python">
```python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key="sk-your-api-key"
)

def stream_chat():
    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": "å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„è¯—"}
        ],
        stream=True,
        max_tokens=300
    )

    print("AIæ­£åœ¨åˆ›ä½œï¼Œè¯·ç¨å€™...")
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="", flush=True)
    
    print("\n\nåˆ›ä½œå®Œæˆï¼")

stream_chat()
```
  </Tab>
  <Tab value="JavaScript">
```javascript
import OpenAI from 'openai';

const client = new OpenAI({
    baseURL: 'https://api.pandalla.ai/v1',
    apiKey: 'sk-your-api-key'
});

async function streamChat() {
    try {
        const stream = await client.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                { role: 'user', content: 'å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„è¯—' }
            ],
            stream: true,
            max_tokens: 300
        });

        console.log('AIæ­£åœ¨åˆ›ä½œï¼Œè¯·ç¨å€™...');
        
        for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content;
            if (content) {
                process.stdout.write(content);
            }
        }
        
        console.log('\n\nåˆ›ä½œå®Œæˆï¼');
    } catch (error) {
        console.error('Error:', error);
    }
}

streamChat();
```
  </Tab>
</Tabs>

## ğŸ¨ å¤šæ¨¡æ€ç¤ºä¾‹

### å›¾åƒç†è§£

<Tabs items={['Python', 'JavaScript']}>
  <Tab value="Python">
```python
import base64
from openai import OpenAI

client = OpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key="sk-your-api-key"
)

def encode_image(image_path):
    """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64æ ¼å¼"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def analyze_image(image_path, question="è¯·æè¿°è¿™å¼ å›¾ç‰‡"):
    """åˆ†æå›¾ç‰‡å†…å®¹"""
    
    # ç¼–ç å›¾ç‰‡
    base64_image = encode_image(image_path)
    
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": question
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        max_tokens=500
    )
    
    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
try:
    result = analyze_image("./example.jpg", "è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ")
    print("å›¾ç‰‡åˆ†æç»“æœï¼š")
    print(result)
except FileNotFoundError:
    print("å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
except Exception as e:
    print(f"åˆ†æå¤±è´¥ï¼š{e}")
```
  </Tab>
  <Tab value="JavaScript">
```javascript
import OpenAI from 'openai';
import fs from 'fs';

const client = new OpenAI({
    baseURL: 'https://api.pandalla.ai/v1',
    apiKey: 'sk-your-api-key'
});

function encodeImage(imagePath) {
    // è¯»å–å¹¶ç¼–ç å›¾ç‰‡
    const imageBuffer = fs.readFileSync(imagePath);
    return imageBuffer.toString('base64');
}

async function analyzeImage(imagePath, question = "è¯·æè¿°è¿™å¼ å›¾ç‰‡") {
    try {
        // ç¼–ç å›¾ç‰‡
        const base64Image = encodeImage(imagePath);
        
        const response = await client.chat.completions.create({
            model: "gpt-4-vision-preview",
            messages: [
                {
                    role: "user",
                    content: [
                        {
                            type: "text",
                            text: question
                        },
                        {
                            type: "image_url",
                            image_url: {
                                url: `data:image/jpeg;base64,${base64Image}`
                            }
                        }
                    ]
                }
            ],
            max_tokens: 500
        });
        
        return response.choices[0].message.content;
        
    } catch (error) {
        throw new Error(`å›¾ç‰‡åˆ†æå¤±è´¥: ${error.message}`);
    }
}

// ä½¿ç”¨ç¤ºä¾‹
analyzeImage('./example.jpg', 'è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ')
    .then(result => {
        console.log('å›¾ç‰‡åˆ†æç»“æœï¼š');
        console.log(result);
    })
    .catch(error => {
        console.error(error.message);
    });
```
  </Tab>
</Tabs>

## ğŸ”„ å¤šæ¨¡å‹å¯¹æ¯”

### åŒæ—¶è°ƒç”¨å¤šä¸ªæ¨¡å‹

```python
from openai import OpenAI
import asyncio
import time

client = OpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key="sk-your-api-key"
)

async def compare_models():
    """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å“åº”"""
    
    prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—çš„æ ¸å¿ƒæ¦‚å¿µ"
    models = ["gpt-4o", "gpt-4o-mini", "claude-3-5-sonnet-20241022", "gemini-1.5-flash-002"]
    
    async def get_response(model):
        """è·å–å•ä¸ªæ¨¡å‹çš„å“åº”"""
        start_time = time.time()
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=0.7
            )
            
            end_time = time.time()
            
            return {
                "model": model,
                "response": response.choices[0].message.content,
                "response_time": round(end_time - start_time, 2),
                "tokens": response.usage.total_tokens if hasattr(response, 'usage') else None
            }
            
        except Exception as e:
            return {
                "model": model,
                "response": f"é”™è¯¯: {str(e)}",
                "response_time": None,
                "tokens": None
            }
    
    # å¹¶å‘è¯·æ±‚æ‰€æœ‰æ¨¡å‹
    print(f"é—®é¢˜: {prompt}")
    print("-" * 80)
    
    tasks = [get_response(model) for model in models]
    results = await asyncio.gather(*tasks)
    
    # æ‰“å°ç»“æœ
    for result in results:
        print(f"\nğŸ¤– æ¨¡å‹: {result['model']}")
        print(f"â±ï¸  å“åº”æ—¶é—´: {result['response_time']}ç§’")
        if result['tokens']:
            print(f"ğŸ”¢ Tokenä½¿ç”¨: {result['tokens']}")
        print(f"ğŸ“ å›ç­”: {result['response']}")
        print("-" * 80)

# è¿è¡Œæ¯”è¾ƒ
asyncio.run(compare_models())
```

## ğŸ¬ è§†é¢‘ç”Ÿæˆç¤ºä¾‹

### Sora è§†é¢‘ç”Ÿæˆ

```python
import asyncio
import time
from openai import AsyncOpenAI

client = AsyncOpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key="sk-your-api-key"
)

async def create_video_with_progress():
    """åˆ›å»ºè§†é¢‘å¹¶æ˜¾ç¤ºè¿›åº¦"""
    
    # åˆ›å»ºè§†é¢‘ä»»åŠ¡
    video = await client.videos.create(
        model="sora-2",
        prompt="ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨é˜³å…‰æ˜åªšçš„èŠ±å›­é‡Œç©è€ï¼ŒèƒŒæ™¯æ˜¯äº”é¢œå…­è‰²çš„èŠ±æœµ",
        seconds=4,
        size="1280x720"
    )
    
    print(f"âœ… è§†é¢‘ä»»åŠ¡å·²åˆ›å»º")
    print(f"ğŸ“¹ Video ID: {video.id}")
    print(f"ğŸ“ æè¿°: {video.prompt}")
    print(f"â³ å¼€å§‹ç”Ÿæˆè§†é¢‘ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # è½®è¯¢æ£€æŸ¥çŠ¶æ€
    while True:
        video_status = await client.videos.retrieve(video.id)
        status = video_status.status
        
        if status == "completed":
            print(f"\nğŸ‰ è§†é¢‘ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ“ å¯ä»¥ä¸‹è½½è§†é¢‘äº†")
            
            # å¯é€‰ï¼šç›´æ¥ä¸‹è½½è§†é¢‘
            # content = await client.videos.download_content(video.id)
            # content.write_to_file(f"{video.id}.mp4")
            # print(f"ğŸ’¾ è§†é¢‘å·²ä¿å­˜ä¸º {video.id}.mp4")
            
            break
        elif status == "failed":
            print(f"\nâŒ è§†é¢‘ç”Ÿæˆå¤±è´¥")
            break
        else:
            # æ˜¾ç¤ºè¿›åº¦åŠ¨ç”»
            for i in range(3):
                print(f"\r{'â³ ç”Ÿæˆä¸­' + '.' * (i + 1):<15}", end="", flush=True)
                await asyncio.sleep(1)
        
        await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡

# è¿è¡Œç¤ºä¾‹
asyncio.run(create_video_with_progress())
```

## ğŸ› ï¸ å®ç”¨å·¥å…·ç±»

### APIå®¢æˆ·ç«¯å°è£…

```python
import os
import logging
from typing import Optional, List, Dict, Any
from openai import OpenAI

class DubrifyClient:
    """Dubrify APIå®¢æˆ·ç«¯å°è£…ç±»ï¼ˆPandalla API åŒæ ·é€‚ç”¨ï¼‰"""

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.api_key = api_key or os.getenv("API_KEY")
        if not self.api_key:
            raise ValueError("APIå¯†é’¥æœªæä¾›ï¼Œè¯·è®¾ç½®API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°")
        
        self.client = OpenAI(
            base_url="https://api.pandalla.ai/v1",
            api_key=self.api_key
        )
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
    
    def chat(self, 
             message: str, 
             model: str = "gpt-4o-mini",
             system_prompt: Optional[str] = None,
             temperature: float = 0.7,
             max_tokens: int = 1000,
             stream: bool = False) -> str:
        """
        ç®€åŒ–çš„èŠå¤©æ¥å£
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            model: ä½¿ç”¨çš„æ¨¡å‹
            system_prompt: ç³»ç»Ÿæç¤º
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
            
        Returns:
            AIçš„å›å¤å†…å®¹
        """
        try:
            messages = []
            
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            
            messages.append({"role": "user", "content": message})
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )
            
            if stream:
                return self._handle_stream(response)
            else:
                return response.choices[0].message.content
                
        except Exception as e:
            self.logger.error(f"èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
            raise
    
    def _handle_stream(self, stream):
        """å¤„ç†æµå¼å“åº”"""
        content = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                delta = chunk.choices[0].delta.content
                content += delta
                print(delta, end="", flush=True)
        print()  # æ¢è¡Œ
        return content
    
    def translate(self, text: str, target_lang: str = "ä¸­æ–‡", model: str = "gpt-4o-mini") -> str:
        """
        ç¿»è¯‘æ–‡æœ¬
        
        Args:
            text: å¾…ç¿»è¯‘çš„æ–‡æœ¬
            target_lang: ç›®æ ‡è¯­è¨€
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            ç¿»è¯‘ç»“æœ
        """
        prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n\n{text}"
        return self.chat(prompt, model=model, temperature=0.3)
    
    def summarize(self, text: str, length: str = "ä¸­ç­‰", model: str = "gpt-4o-mini") -> str:
        """
        æ–‡æœ¬æ‘˜è¦
        
        Args:
            text: å¾…æ‘˜è¦çš„æ–‡æœ¬
            length: æ‘˜è¦é•¿åº¦ï¼ˆç®€çŸ­/ä¸­ç­‰/è¯¦ç»†ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            æ‘˜è¦ç»“æœ
        """
        length_map = {
            "ç®€çŸ­": "ç”¨1-2å¥è¯",
            "ä¸­ç­‰": "ç”¨3-5å¥è¯", 
            "è¯¦ç»†": "ç”¨ä¸€æ®µè¯"
        }
        
        length_desc = length_map.get(length, "ç”¨3-5å¥è¯")
        prompt = f"è¯·{length_desc}æ€»ç»“ä»¥ä¸‹å†…å®¹çš„è¦ç‚¹ï¼š\n\n{text}"
        
        return self.chat(prompt, model=model, temperature=0.5)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    dubrify = DubrifyClient()

    # ç®€å•èŠå¤©
    response = dubrify.chat("è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ")
    print("AIå›å¤:", response)

    # ç¿»è¯‘
    translation = dubrify.translate("Hello, how are you today?", "ä¸­æ–‡")
    print("ç¿»è¯‘ç»“æœ:", translation)
    
    # æ‘˜è¦
    long_text = """
    äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
    è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ã€ç†è§£è‡ªç„¶è¯­è¨€ä»¥åŠè§£å†³é—®é¢˜ã€‚AIçš„å‘å±•ç»å†äº†å¤šä¸ªé˜¶æ®µï¼Œ
    ä»æ—©æœŸçš„ç¬¦å·AIåˆ°ç°ä»£çš„æ·±åº¦å­¦ä¹ ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ
    æ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸ
    å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
    """
    summary = dubrify.summarize(long_text, "ç®€çŸ­")
    print("æ‘˜è¦:", summary)
```

## ğŸš¨ é”™è¯¯å¤„ç†ç¤ºä¾‹

### å®Œæ•´çš„é”™è¯¯å¤„ç†

```python
import time
import random
from openai import OpenAI
from typing import Optional

client = OpenAI(
    base_url="https://api.pandalla.ai/v1",
    api_key="sk-your-api-key"
)

def robust_chat_with_retry(
    messages: list,
    model: str = "gpt-4o-mini",
    max_retries: int = 3,
    base_delay: float = 1.0
) -> Optional[str]:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„èŠå¤©è¯·æ±‚
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹åç§°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´
        
    Returns:
        AIå›å¤æˆ–Noneï¼ˆå¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼‰
    """
    
    for attempt in range(max_retries + 1):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            error_message = str(e)
            
            # æ£€æŸ¥é”™è¯¯ç±»å‹
            if "401" in error_message:
                print("âŒ APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥å¯†é’¥è®¾ç½®")
                return None
                
            elif "403" in error_message:
                print("âŒ æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥è´¦æˆ·ä½™é¢æˆ–æƒé™")
                return None
                
            elif "404" in error_message:
                print(f"âŒ æ¨¡å‹ {model} ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨")
                return None
                
            elif "429" in error_message:
                if attempt < max_retries:
                    # é€Ÿç‡é™åˆ¶ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    print(f"â³ é€Ÿç‡é™åˆ¶ï¼Œ{delay:.1f}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(delay)
                    continue
                else:
                    print("âŒ è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œé‡è¯•æ¬¡æ•°å·²ç”¨å®Œ")
                    return None
                    
            elif "500" in error_message or "503" in error_message:
                if attempt < max_retries:
                    # æœåŠ¡å™¨é”™è¯¯ï¼Œé‡è¯•
                    delay = base_delay * (attempt + 1)
                    print(f"ğŸ”„ æœåŠ¡å™¨é”™è¯¯ï¼Œ{delay}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(delay)
                    continue
                else:
                    print("âŒ æœåŠ¡å™¨é”™è¯¯ï¼Œé‡è¯•æ¬¡æ•°å·²ç”¨å®Œ")
                    return None
                    
            else:
                print(f"âŒ æœªçŸ¥é”™è¯¯: {error_message}")
                if attempt < max_retries:
                    delay = base_delay
                    print(f"ğŸ”„ {delay}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(delay)
                    continue
                else:
                    return None
    
    return None

# ä½¿ç”¨ç¤ºä¾‹
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"},
    {"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"}
]

result = robust_chat_with_retry(messages)
if result:
    print("âœ… è¯·æ±‚æˆåŠŸ:")
    print(result)
else:
    print("âŒ è¯·æ±‚æœ€ç»ˆå¤±è´¥")
```

<Callout type="info">
è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†Dubrify APIçš„å„ç§ä½¿ç”¨åœºæ™¯ã€‚ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚è°ƒæ•´å’Œæ‰©å±•è¿™äº›ä»£ç ã€‚æ›´å¤šç¤ºä¾‹å’Œæœ€æ–°åŠŸèƒ½ï¼Œè¯·å‚è€ƒ [APIå‚è€ƒæ–‡æ¡£](/docs/api-reference)ã€‚
</Callout>

## ğŸ“š æ›´å¤šèµ„æº

- [API ä½¿ç”¨æŒ‡å—](/docs/guide) - æ·±å…¥äº†è§£APIåŠŸèƒ½
- [Sora è§†é¢‘ç”Ÿæˆ](/docs/sora) - è§†é¢‘ç”Ÿæˆä¸“é¢˜
- [èº«ä»½è®¤è¯](/docs/authentication) - APIå¯†é’¥ç®¡ç†
- [API å‚è€ƒ](/docs/api-reference) - å®Œæ•´çš„APIæ–‡æ¡£